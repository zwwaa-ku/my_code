{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv1D, MaxPooling1D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y_data(inputPath='Personality.xlsx'):\n",
    "    #return nparray: list['Neuroticism (t score)','Extraversion',\n",
    "    #     'Openness','Agreeableness','Conscienciousness']*'num of people'\n",
    "    print('[INFO] loading Personality Data, Path: '+ inputPath)\n",
    "    personality_in1 = pd.read_excel(inputPath)\n",
    "    personality = personality_in1.drop([0])\n",
    "    list_personality = personality[['Neuroticism (t score)','Extraversion (t score)',\\\n",
    "             'Openness (t score)','Agreeableness (t score)','Conscienciousness (t score)','Sex']].values.tolist()\n",
    "    nparray_personality = np.array(list_personality)\n",
    "    return nparray_personality\n",
    "\n",
    "def load_x_data(inputPath):\n",
    "    print('[INFO] loading Acc Data, Path: '+ inputPath)\n",
    "    x_data = []\n",
    "    for u in range(2,53):\n",
    "        acc = pd.read_csv(os.path.join(inputPath,f'{u}','ACC.csv'))\n",
    "        #acc[0:1]はHzを表しているので除く\n",
    "        x_data += [acc[1:]]\n",
    "    #返り値：list、中がDataFrame\n",
    "    return x_data\n",
    "\n",
    "def split_data(data, size):\n",
    "    return_list = []\n",
    "    block_num = len(data) // size\n",
    "    for k in range(0,block_num):\n",
    "        return_list += [np.array(data[k*size:(k+1)*size])]\n",
    "    #返り値：nparray\n",
    "    return np.array(return_list)\n",
    "\n",
    "def mk_data(x, y, size = 10000, number = 3, lock = 1):\n",
    "    #x,y -> load_x_data,load_y_data\n",
    "    xlist = []\n",
    "    ylist = []\n",
    "    for k in range(0,len(x)):\n",
    "        if len(x[k]) < size * number:\n",
    "            print('ValueError: x[k] must be more than size*number')\n",
    "            break\n",
    "        splited = split_data(x[k], size)\n",
    "        index = list(itertools.permutations(range(splited.shape[0]), number))\n",
    "        random.shuffle(index)\n",
    "        upper = lock\n",
    "        #print(upper)\n",
    "        for l in index[:upper]:\n",
    "            onedata = splited[list(l)]\n",
    "            xlist += [onedata.reshape(size*number,3)]\n",
    "            ylist += [y[k]]\n",
    "    re_x = np.array(xlist)\n",
    "    re_y = np.array(ylist)\n",
    "    return re_x, re_y\n",
    "\n",
    "def data_shuffle(x_b, y_b):\n",
    "    x_list = []\n",
    "    index = np.random.permutation(len(x_b))\n",
    "#    index =[29,43,7,24,26,40,12,47,20,16,49,14,41,15,48,10,19,42,13,1,6,8,4,25,23,35,34,2,21,0,31,46,32,30,18,17,45,36,37,50,11,3,38,9,5,39,44,33,22,28,27]\n",
    "    for i in index:\n",
    "        x_list += [x_b[i]]\n",
    "    y_b = y_b[index]\n",
    "    return x_list, y_b, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading Personality Data, Path: ../Personality_v2.xlsx\n",
      "[INFO] loading Acc Data, Path: ../E4wristband\n"
     ]
    }
   ],
   "source": [
    "#var\n",
    "counter = 2\n",
    "fivefactor = 0\n",
    "#0:Nt, 1:Et, 2:Ot, 3:At, 4:Ct\n",
    "split_use_num = 5\n",
    "epoch = 300\n",
    "\n",
    "#finalvar\n",
    "covlist = []\n",
    "traindata_size = 100000\n",
    "\n",
    "split = int(traindata_size/split_use_num)\n",
    "if traindata_size%split_use_num != 0:\n",
    "    print('split_use_num was individabl')\n",
    "\n",
    "#preprocess\n",
    "personality = load_y_data('../Personality_v2.xlsx')\n",
    "#print(personality)\n",
    "\n",
    "directoryname = {0:'Nt', 1:'Et', 2:'Ot', 3:'At', 4:'Ct'}\n",
    "# t-scoreを０−１に落とす、平均０.５\n",
    "max_values = (np.abs(personality[:, fivefactor] - 50)).max()\n",
    "y_train_before = (personality[:, fivefactor] - 50)/(2*max_values) + 0.5\n",
    "\n",
    "x_train_before = load_x_data('../E4wristband')\n",
    "#print(y_train_before.shape)\n",
    "\n",
    "indexlist = np.empty((counter, y_train_before.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_0 (Conv1D)            (None, 100000, 32)        800       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100000, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_0 (MaxPooling1 (None, 33333, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 33333, 64)         4160      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 33333, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11111, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 11111, 128)        16512     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 11111, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3703, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 473984)            0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 100)               47398500  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 47,465,353\n",
      "Trainable params: 47,465,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 7s 231ms/step - loss: 0.1630 - val_loss: 0.2244\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 6s 214ms/step - loss: 0.1065 - val_loss: 0.1050\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 6s 214ms/step - loss: 0.0355 - val_loss: 0.0580\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 6s 214ms/step - loss: 0.0221 - val_loss: 0.0515\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 6s 215ms/step - loss: 0.0214 - val_loss: 0.0514\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 6s 216ms/step - loss: 0.0214 - val_loss: 0.0504\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 6s 215ms/step - loss: 0.0214 - val_loss: 0.0507\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 6s 216ms/step - loss: 0.0216 - val_loss: 0.0503\n",
      "Epoch 9/10\n",
      "19/29 [==================>...........] - ETA: 2s - loss: 0.0209"
     ]
    }
   ],
   "source": [
    "shuffle_index = np.empty(y_train_before.shape[0])\n",
    "boader=30\n",
    "x_train=np.empty((boader,traindata_size,3))\n",
    "y_train=np.empty(boader)\n",
    "x_test=np.empty(((y_train_before.shape[0]-boader),traindata_size,3))\n",
    "y_test=np.empty(y_train_before.shape[0]-boader)\n",
    "for repeat in range(counter):\n",
    "    x_all, y_all, shuffle_index = data_shuffle(x_train_before[:], y_train_before[:])\n",
    "\n",
    "    \n",
    "    x_train,y_train = mk_data(x_all[:boader], y_all[:boader], split, split_use_num, 30)\n",
    "    x_test, y_test = mk_data(x_all[boader:], y_all[boader:], traindata_size, 1, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 8, padding=\"same\", input_shape = (x_train.shape[1], 3), activation = 'relu', name='conv1d_0'))\n",
    "    model.add(Conv1D(32, 4, padding=\"same\", activation='relu', name='conv1d_1'))\n",
    "    model.add(MaxPooling1D(3, name='max_pooling1d_0'))\n",
    "    model.add(Conv1D(64, 2, padding=\"same\", activation='relu', name='conv1d_2'))\n",
    "    model.add(Conv1D(64, 2, padding=\"same\", activation='relu', name='conv1d_3'))\n",
    "    model.add(MaxPooling1D(3, name='max_pooling1d_1'))\n",
    "    model.add(Conv1D(128, 2, padding=\"same\", activation='relu', name='conv1d_4'))\n",
    "    model.add(Conv1D(128, 2, padding=\"same\", activation='relu', name='conv1d_5'))\n",
    "    model.add(MaxPooling1D(3, name='max_pooling1d_2'))\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(100, activation='sigmoid', name='dense_0'))\n",
    "    model.add(Dense(1, activation='sigmoid', name='dense_1'))\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "    model.summary()\n",
    "    learning = model.fit(x_train, y_train, epochs=epoch, validation_data = (x_test,y_test))\n",
    "\n",
    "    print('finish '+str(repeat)+'times learning')\n",
    "    result = model.predict(x_test).flatten()\n",
    "    cov = np.corrcoef(result, y_test)\n",
    "    print('共分散 : '+ str(cov[0][1]))\n",
    "    covlist += [cov[0][1]]\n",
    "    \n",
    "    indexlist[repeat] = shuffle_index[:]\n",
    "    \n",
    "    if repeat==0:\n",
    "        jsonfilename=directoryname[fivefactor]+'_cnn_model.json'\n",
    "        json_string = model.to_json()\n",
    "        open(os.path.join('../data',directoryname[fivefactor],jsonfilename), 'w').write(json_string)\n",
    "    \n",
    "    hdf5filename=str(repeat)+'cnn_model_weights.hdf5'\n",
    "    model.save_weights(os.path.join('../data',directoryname[fivefactor],hdf5filename))\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()\n",
    "    #del x_test\n",
    "    #del y_test\n",
    "    #del x_train\n",
    "    #del y_train\n",
    "    del x_all\n",
    "    del y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[[9.88e-324 1.48e-323 1.98e-323 2.47e-323 2.96e-323 3.46e-323 3.95e-323\n",
      "  4.45e-323 4.94e-323 5.43e-323 5.93e-323 6.42e-323 6.92e-323 7.41e-323\n",
      "  7.91e-323 8.40e-323 8.89e-323 9.39e-323 9.88e-323 1.04e-322 1.09e-322\n",
      "  1.14e-322 1.19e-322 1.24e-322 1.28e-322 1.33e-322 1.38e-322 1.43e-322\n",
      "  1.48e-322 1.53e-322 1.58e-322 1.63e-322 1.68e-322 1.73e-322 1.78e-322\n",
      "  1.83e-322 1.88e-322 1.93e-322 1.98e-322 2.03e-322 2.08e-322 2.12e-322\n",
      "  2.17e-322 2.22e-322 2.27e-322 2.32e-322 2.37e-322 2.42e-322 2.47e-322\n",
      "  2.52e-322 2.57e-322]\n",
      " [4.94e-324 4.94e-324 4.94e-324 4.94e-324 9.88e-324 9.88e-324 4.94e-324\n",
      "  9.88e-324 4.94e-324 9.88e-324 4.94e-324 4.94e-324 4.94e-324 9.88e-324\n",
      "  4.94e-324 4.94e-324 9.88e-324 9.88e-324 4.94e-324 9.88e-324 4.94e-324\n",
      "  9.88e-324 9.88e-324 9.88e-324 9.88e-324 4.94e-324 9.88e-324 9.88e-324\n",
      "  9.88e-324 4.94e-324 4.94e-324 9.88e-324 4.94e-324 4.94e-324 9.88e-324\n",
      "  4.94e-324 9.88e-324 4.94e-324 4.94e-324 4.94e-324 9.88e-324 9.88e-324\n",
      "  9.88e-324 9.88e-324 9.88e-324 4.94e-324 4.94e-324 4.94e-324 4.94e-324\n",
      "  9.88e-324 9.88e-324]]\n",
      "save convlist and indexlist\n",
      "Nt\n"
     ]
    }
   ],
   "source": [
    "print(covlist)\n",
    "print(indexlist)\n",
    "print('save convlist and indexlist')\n",
    "#print(type(indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/'+ directoryname[fivefactor]+'/cov.csv', 'w') as j:\n",
    "    writer = csv.writer(j)\n",
    "    writer.writerows(covlist)\n",
    "np.savetxt('..data/'+directoryname[fivefactor]+'/indexlist.csv', indexlist, delimiter=\",\", fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-Keras",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
